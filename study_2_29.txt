2/29 report
决策树学习
决策树分两个阶段
1、 训练阶段
从给定的训练数据集DB，构造出一颗决策树
class=DecisionTree(DB)
2、 分类阶段
从根开始，按照决策树的分类属性逐层往下划分，直到叶节点，获得概念（决策、分类）结果。
y=DecisionTree(x)
熵的概念
代表的是一个混乱程度
假设X和Y两个事件相互独立，则P(X,Y)=P(X)*P(Y)，Log(XY)=Log(X)+Log(Y)
H(X),H(Y)：事件发生的不确定性
P(几率越大)->H(X)值越小->熵值越小
P(几率越小)->H(X)值越大->熵值越大


0<P<1，最终熵值是大于0的。

尼基系数和熵值的定义类似，尼基系数越大，熵值也越大，说明越混乱。

构造决策树根节点
决策树的基本思想是随着树深度的增加，节点的熵迅速地降低。熵降低的速度越快越好，这样我们构造的决策树是一颗高度最矮的决策树（分支过多容易出现过拟合的现象）。
决策树算法
ID3:信息增益
C4.5:信息增益率
CART：Gini系数
ID3算法流程
1)初始化信息增益的阈值。
2）判断样本是否为同一类输出，如果是则返回单节点树。标记类别为。
3) 判断特征是否为空，如果是则返回单节点树，标记类别为样本中输出类别实例数最多的类别。
4）计算中的各个特征（一共个）对输出D的信息增益，选择信息增益最大的特征。
5) 如果的信息增益小于阈值，则返回单节点树，标记类别为样本中输出类别实例数最多的类别。
6）否则，按特征的不同取值将对应的样本输出分成不同的类别。每个类别产生一个子节点。对应特征值为。返回增加了节点的数。
7）对于所有的子节点，令, 递归调用2-6步，得到子树并返回。
